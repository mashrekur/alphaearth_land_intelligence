# Supplementary Materials

## LLM-as-Judge Evaluation Examples

**File:** `supplementary_evaluation_examples.csv`

This file contains 30 representative query–response–judgment cycles from the LLM-as-Judge evaluation experiment described in the manuscript (Section 3.5). The full evaluation comprised 360 cycles across 12 configurations of four LLMs in rotating generator, system, and judge roles.

### Selection Criteria

Examples were stratified to provide balanced coverage across:

- **Intent types**: 3 examples per intent (10 intent types × 3 = 30), spanning location profiling, flood risk, drought risk, vegetation health, climate summary, land use, soil properties, hydrology, temporal change, and comparison queries.
- **Score range**: For each intent type, one low-scoring, one mid-scoring, and one high-scoring example were selected (overall range: 1.30–4.40 on the 1–5 scale).
- **System models**: Approximately balanced across all four LLM backends (GPT-OSS-120B, Llama-3.2-11B, Gemma-3-27B, Qwen3-VL-32B).

### Column Descriptions

| Column | Description |
|--------|-------------|
| `cycle_id` | Unique evaluation cycle identifier |
| `intent_type` | Query intent category (one of 10 types) |
| `query_text` | Natural language query generated by the Query Generator LLM |
| `location_name` | Resolved location name or coordinate reference |
| `longitude` | Query longitude (decimal degrees) |
| `latitude` | Query latitude (decimal degrees) |
| `year` | Target data year (2017–2023) |
| `system_model` | LLM backend used by the Land Surface Intelligence system |
| `generator_model` | LLM used to generate the query |
| `judge_model` | LLM used to evaluate the response |
| `response_text` | Full system response |
| `grounding` | Judge score (1–5): References to actual embedding data and environmental variables |
| `scientific_accuracy` | Judge score (1–5): Consistency with validated dimension–variable relationships |
| `completeness` | Judge score (1–5): Coverage of query-relevant environmental categories |
| `coherence` | Judge score (1–5): Structure, clarity, and logical organization |
| `practical_utility` | Judge score (1–5): Actionability for environmental decision-making |
| `weighted_score` | Weighted composite score (grounding 0.25, accuracy 0.25, completeness 0.20, coherence 0.15, utility 0.15) |
| `judge_reasoning` | Judge's free-text rationale for the assigned scores |

### Citation

If using this data, please cite the main manuscript and the archived repository (DOI: 10.5281/zenodo.18566431).
